---
  title: 'Support Vector Machines'
  output: rmarkdown::github_document
---

  ```{r setup, include=FALSE}
rm(list = ls(all = TRUE))
libs <- c("tidyverse", "ISLR", "modelr", "e1071", "ROCR")
invisible(lapply(libs, library, character.only = TRUE))
```

The support vector machine (SVM) is a group of classification algorithms that include a variety of parametric and non-parametric models. The parametric models include straight lines and similar regression lines. The non-parametric models include a variety of kernel smoothing techniques.

The support vector machine is a generalization of maximum margin classifier. The maximum margin classifer that classifies the observation using the maximal margin hyperplane which is the seperating hyperplane that is farthest from the observations. The support vectors are the points (vectors in $p$-dimensional space) which "support" the maximal margin hyperplane in the sens that if these points were moved slightly then the maximal margin hyperplane would move as well. The maximal margin hyperplane depends directly on the support vectos, but not on the other observations.

## The advantages of support vector machines

* Effective in high dimensional spaces.

* Still effective in cases where number of dimensions is greater than the number of samples.

* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.

* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.

## The disadvantages of support vector machines

* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.

* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities).

  ```{r, include=FALSE}
data<-read.csv(file="WDBC.csv", header=TRUE)
```